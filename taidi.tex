\documentclass[UTF8]{ctexart}
\title{题目}
\author{林一凡\quad王佳璇\quad万宗祺}
\date{\today}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsmath}
\geometry{left = 3.5cm,right=3.5cm,top=2.5cm,bottom=2.5cm}
\usepackage{cite}
\begin{document}
\fontsize{12pt}{24pt}
\maketitle
\begin{abstract}
\begin{flushleft}
\textbf{关键词：} RNN;LSTM;中文分词;词嵌入
\end{flushleft}
\end{abstract}

\tableofcontents
\section{引言}
近年来，随着互联网产业的迅猛发展，人们接触到的信息量越来越庞大，对高效的信息检索能力的需求日益旺盛。对一篇文本，我们希望不必亲自阅读就能直接得到我们想要的问题的答案所在的句子或者段落，要达到这个目的，仅仅靠简单的词语匹配是不行的，这促使了问答系统研究的产生与发展，在智能阅读模型中，我们要做的是对候选答案句进行确认，标记出正确答案所在的句子，中文候选答案句确认的研究进展目前不及英文候选答案句确认的进展，因为中文句子在分词上面比英文句子难度大得多，而且中文的语法比英文更复杂，表达方式也更加丰富，这些都是中文问答系统的难点。

早期的答案抽取研究一般是基于规则的答案匹配，以及提取特征并利用传统机器学习算法进行监督学习，而在近几年，由于深度学习的兴起，利用深度学习解决答案抽取问题成为了主流方法，它可以绕过人工构造特征的繁琐过程，自动学习到自然语言中的深层特征，从而正确地识别出正确答案所在句子。

本文阐述我们如何利用传统机器学习和LSTM等模型构建一个智能阅读系统。
\section{方案制定}
对智能阅读模型中候选答案选取的研究，我们想到从三个方向入手：人工制定匹配规则，根据句子语义，句子语法结构构建特征使用传统机器学习方法进行而分类，深度学习方法。下面我们依次分析这三个方向的可行性。
\subsection{人工制定匹配规则}
在这种方法下，研究者一般采用句模匹配方法，该方法考虑答案出现的上下文信息，在问题类型确定的前提下，调用与此类型问题相关的答案模板来抽取答案，此种方法依赖于对问题的精确而且精细的分类，而且人工制定匹配规则需要消耗大量人力以及时间，即使这样，对于大量复杂的问题，仍然无法正确地抽取答案。%孙昂

因为这些弊端的存在，在统计学习方法成为主流的现在，很少看到通过这种方法来进行研究的了，由于数据集涉及的问答对范围之广，不仅仅局限在某一个特定领域，即使我们有足够的人力和时间进行匹配规则的制定，也难以在这个数据集上取得好的效果。所以我们直接放弃了这种方法。
\subsection{传统机器学习方法}
\subsubsection{词义特征}
对于传统机器学习方法，可以用来分类数据的特征有词义特征以及句法特征。最简单的例子是词袋模型，即计算问句与答句中相同的词语占句子总长度的比例（词共现特征）：$$wordbagSim=\frac{2*N}{length(Q)+length(A)}$$其中N是问句Q和答句A中相同词语的数量。词袋模型能够比较低效的表示两个句子的相似程度，从而筛选出与问句主题差不多的答句，然而句子在很大程度上还受到词序，语法的影响，倘若一个句子和问句一模一样，那么它虽然词共现比例达到了1，但是它很明显不是问题的答案。

基于n-gram语法假设，我们可以设置其它的词共现特征，比如bi-gram词共现特征，即计算问句与答句中共同出现的两个连续词语构成的词对出现的次数占句子总长度的比例，高元的词共现特征能体现两个句子更高的一致性，但是过高元的词共现特征又是没有意义的。一般常用uni-gram，bi-gram，tri-gram词共现特征。在本文中我们选取了unigram与bigram词共现特征。

词共现特征即使仅仅在词义的表达上也是不完善的，因为它没有考虑到同义词，这一方面有不少解决方案，比如哈工大的同义词词林，以及WordNet知网，这些人工编制的词典使得我们可以根据词语在词典中所属的位置来计算两个词语的相似程度。由此可以改进词共现特征，加入词汇相似度，有如下衡量两个句子相似度的公式：$$Sim =\frac{\frac{\sum^n_{i=1}\sum_{j=1}^m argmax Sim_{ij}}{n}+\frac{\sum^n_{i=1}\sum_{j=1}^m argmax Sim_{ij}}{m}}{2}$$
即对于A、B两个文本，对于A文本中的每个词，找出B文本中与之相似度最高的词，统计A文本中所有词的平均值作为A与B的相似度，同理再求出B与A的相似度，二者再求平均值，即为最终的文本相似度。要应用这些词义相似度量特征，需要先对句子做去除停用词处理，因为停用词在很多句子中都会出现。%看情况处理这一段，如果篇幅不够就把这个单独做一章节，作为基准模型
%%下面放部分停用词的表
\subsubsection{句法特征}
句法特征也是影响句子含义的非常重要的部分，同一个词语在不同的句子里面所担任的角色很可能是不同的，因此对语义的影响也是不同的。

依存句法分析是提取句法特征的一个工具。依存文法是由法国语言学家L.Te\\siniere在1959年提出来的，该文法认为句子中述语动词是支配其它成分的中心，而它本身却不受其它成分的支配，所有受支配者都以某种依存关系从属于其支配者，比如OBJ（宾语），ATT（定中结构），SBJ（主语）等等。%基于句法结构分析的中文问题分类

哈工大的ltp自然语言处理工具提供了依存关系分析工具，我们在使用传统机器学习方法的时候用到了ltp提供的依存分析工具。利用该工具，我们能对比较简单的句子识别出它的基本语法结构，比如句子的谓语，主语，宾语，并列结构等等。

有了句子依存的依存关系，我们可以依次提取出能够表示句法的特征。

通过依存树来度量问句与答案句的相似度，我们想到用子树匹配的方法，即问题句的依存树结构是否是答案句依存树的一个子树。这样的想法是自然的，因为我们发现，中文问句大部分的形式都是将陈述句等等形式的句子中的答案词替换为疑问代词得到的。比如：“白发魔女是李冰冰扮演的”一句，将李冰冰替换为谁就得到“白发魔女是谁扮演的”，经过替换后这两句话的依存结构完全没有发生变化，仅仅是李冰冰被替换为谁，因此我们认为这是一个非常有效的特征。

又由于训练集答案句普遍较长，含有大量与问句无关的内容，而只要出现了答案模式，如上例中“白发魔女是xx扮演的”，就意味着这一句应该被标记为正例。所以对于依存子树特征，只要答案句依存树匹配到了问句依存树，则使该特征取值为1。匹配的方法就是传统的子树匹配，但是需要注意一点，问句中的疑问代词在匹配中相当于一个“通配符”，它可以与任何与其对应的词性的词语匹配，下面给出不同疑问代词的词性对应表。%给表

在对问句与答句的人工观察中，我们发现，中文表达中一种倒装的形式经常出现，比如“白发魔女是李冰冰扮演的”和“李冰冰扮演白发魔女”，两个句子的意思是一样的，但是它们的词序以及依存结构都有很大的不同，下面是它们的依存关系示意图：%给出依存关系图

这样的情况是普遍存在的，为了让它们不对依存树的匹配以及句法特征产生干扰，我们对所有依存树做如下处理：若有动词依存于“是”，那么就让“是”下属的依存弧连接到这个动词，并将该动词依存于“是”所依存的词，再从依存树中删去“是”。比如上例，倒装句经过所述的修改后，依存关系变为下图：%给出修改后依存图

可以看出，现在它们的依存关系是相似的了，经过我们的抽样调查，这样的操作对绝大部分的倒装情形都能起效果。

文献xx（给出文献）提出了基于依存树的另一个特征，即有效搭配词特征，搭配词是指依存弧两端的词语，如上例的“李冰冰扮演白发魔女”中，（扮演，李冰冰）就是一个搭配词对，然而更多句子的依存树中存在着很多其他的依存关系，
\subsection{深度学习方法}

\section{数据预处理}
\subsection{数据均衡化}
经过初步统计，我们发现，训练集中的答案句有28\%是正确答案句，剩下的72\%是错误答案句，正反例的不平衡无疑会对训练模型产生非常不好的影响，模型可能会将所有的句子全部判定为错误答案句，这样就已经能够取得72\%的正确率了。

常见的处理正负例不均衡的方法有过采样法和降采样法。过采样即随机地重复采样正例从而使正负例数量相同，而降采样就是把一部分负例从训练集中去除，从而使得正例与负例达到均衡。

考虑到如果采取降采样法，将会损失大量数据，所以我们采取了过采样法。
\subsection{句子的分词}
在中文句子中，一个“语义”的单元是一个词语，这意味着我们需要首先把句子分解成词语，这方面的研究已经发展的比较完善了，目前已经有几个开放的自然语言处理工具能在分词任务上取得不错的效果了，比如结巴分词，哈工大自然语言处理工具ltp，中科院分词工具ICTCLAS。我们选择了结巴分词来处理数据。
\subsection{句子的向量表示}
经过分词之后，句子可以看作是词语的序列，对句子的向量表示就是对词语的向量表示问题。

词语最简单的表示方法就是one-hot向量，它将词典的长度作为向量的长度，而只在该向量对应词在词典中的编号对应的向量位置标为1，其它位置都标为0。例如词语“人群”在词典的第1个位置，那么它对应的one-hot向量表示就为[1,0,0,...]。

one-hot向量虽然能简单地完成对词语的表示任务，但是它存在两个弊端，一个是它会出现“词汇鸿沟”现象，即将不同的词语判定为完全不同，譬如“土豆”与“马铃薯”，它们本来有同一个含义，但是在one-hot向量表示中，它们却是完全不同的，没有任何联系，这无疑会造成信息的大量丢失。第二个弊端是它的向量维度太过于庞大，词典中要包含训练集中所有的词语，那么one-hot向量可能会是几万甚至几十万维，这样庞大的维度不仅在空间还是时间上都将产生严重的性能损失。

而另一种表示方法--分布式表示就能比较好的解决这两个弊端。分布式表示的词向量基于一个假设：相似的词语有着相似的上下文信息。因此，该方法主要是根据词汇在文本中的上下文信息来获得词语的向量表示，最终将词语表示成一个固定维数的连续数值向量。这样使得词向量之间可以计算相似度，并且使得相似的词或者同义词相似度更高，解决了one-hot向量的“词汇鸿沟”现象，同时由于分布式表示把词语表示为低维的--通常为50，100维的向量，数据维数大大减少，节省了大量内存与计算资源。

在本文中，考虑到训练分布式词向量需要大规模的语料，我们使用了已经基于共120g中文语料（包括新闻，百科，小说）训练好的预训练word2vec词向量。
\subsection{特殊符号的处理}
训练集中除了中文之外，还有一些非中文符号，譬如标点符号，以及英文词语，小语种词语，以及数据获取过程中从网页上爬取的html标签。

对于符号，它们对句子的语义没有太大贡献，但是它们隐藏着关于分词的信息，若两个字之间有逗号，那么他们就不能被判定为是一个词语。所以我们先将句子分词，再直接删去这些符号，然后再将句子转换为向量序列。

对于word2vec词典中没有的词语，我们把它们分配给同一个向量，我们发现这样的词语并不多，因此也不会对最终结果产生大的影响。

对于阿拉伯数字，词典中没有，但是它们的意义是显然的，而且在数据集中的数量也不算少，我们不能够像上一段中的词语那样处理这些数字，而必须将它们与别的词语和字符区分开，并且多少要保留数字的意义，经过考虑，我们将它们转换成为中文的数字，比如186就转换为一八六。

\end{document}